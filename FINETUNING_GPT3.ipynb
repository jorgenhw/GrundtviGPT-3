{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# *INSTRUCTIONS*\n",
        "The following notebook contains the code required to fine-tune GPT-3 on the Danish *Højskolesangbog* as part of the project **GrundtviGPT-3**.\n",
        "\n",
        "For detailed instructions, we refer to the readme-file located in the github repository https://github.com/jorgenhw/GrundtviGPT-3.\n",
        "\n",
        "NB: Step 3.3 can be skipped since the preprocessed and formatted .jsonl file is loaded into the notebook in step 4."
      ],
      "metadata": {
        "id": "MobV67qjKVmC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Imports"
      ],
      "metadata": {
        "id": "JkIHKzozAt74"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "By1P-4qS5uou",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b7626ca-6101-4f10-8f1b-dd8d79baf902"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l\r\u001b[K     |███████▎                        | 10 kB 25.9 MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 20 kB 28.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 30 kB 33.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 40 kB 37.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 44 kB 2.2 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 147 kB 9.0 MB/s \n",
            "\u001b[?25h  Building wheel for openai (PEP 517) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade openai --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. API key\n",
        "Here you need to insert your own API key which you can get from https://beta.openai.com/."
      ],
      "metadata": {
        "id": "Awy-lNewAyny"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"]=(\"YOUR_API_KEY_GOES_HERE\")"
      ],
      "metadata": {
        "id": "LurupC9W5zoQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Data"
      ],
      "metadata": {
        "id": "YKVr0VDtTylH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.1 Loading data"
      ],
      "metadata": {
        "id": "sSGv2HF3X4wG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"https://raw.githubusercontent.com/jorgenhw/GrundtviGPT-3/main/data/sangtekster.csv\", sep = \";\") # links to github repository"
      ],
      "metadata": {
        "id": "Tg2WSdoTGTmU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.2 Preprocessing"
      ],
      "metadata": {
        "id": "FJOzNXj3Gj_T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.drop(\"ID\", axis = 1) # dropping original ID column (was incorrect)\n",
        "df['text'] = df['text'].str[4:] # deleting 4 first characters to remove first number from each song\n",
        "\n",
        "# Function extracts the first characters of a song until the first number occurs (indicating the beginning og verse 2). Function is used to extract first verse of all songs\n",
        "def extract_until_number(s):\n",
        "    result = \"\"\n",
        "    for c in s:\n",
        "        if c.isnumeric():\n",
        "            break\n",
        "        result += c\n",
        "    return result\n",
        "\n",
        "\n",
        "# Apply the above function to the text column in the df\n",
        "df[\"prompt\"] = df[\"text\"].apply(extract_until_number)\n",
        "\n",
        "# Delete the extracted characters from the \"text\" column\n",
        "df[\"text\"] = df.apply(lambda row: row[\"text\"].replace(row[\"prompt\"], \"\"), axis=1)\n",
        "\n",
        "# Renaming columns\n",
        "df = df.rename(columns={\"text\": \"completion\"})\n",
        "\n",
        "# Reordering columns\n",
        "df = df.reindex(columns=['prompt', 'completion'])"
      ],
      "metadata": {
        "id": "_L_G1oeF8x0I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Following songs are dropped from the dataset since we use the first verses from these as prompts**\n",
        "* 311 (yndigt at følges ad), \n",
        "* 170 (hvidt herude), \n",
        "* 136 (der er noget i luften), \n",
        "* 132 (vær velkommen),  \n",
        "* 301 (vem kan segla)\n",
        "* 220 (danmark nu blunder)\n",
        "* 140 (en rose så jeg skyde)\n",
        "* 62 (jens vejmand)\n",
        "* 305 (det var en lørdag aften)\n",
        "* 321 (dansevise)\n",
        "* 37 (amazing grace)\n",
        "\n",
        "NB: Number refers to index number, not actual song number"
      ],
      "metadata": {
        "id": "dUzpkxqMH8su"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "songs = df.iloc[[311,170,136,132,301,220,140,62,305,321,37]]\n",
        "prompts = songs[\"prompt\"].tolist()\n",
        "\n",
        "df = df.drop([311, 170,136,132,301,220,140,62,305,321,37])"
      ],
      "metadata": {
        "id": "xRtR6uZTJqVP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Length of dataframe after pre-processing:"
      ],
      "metadata": {
        "id": "FQ-ivfJkIeQp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LzRnfwJlfoYZ",
        "outputId": "6f8ea3f0-0237-4994-90e9-c410659c920d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(402, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Optional: Save file locally (remoce '#' from line 2 and 3)*"
      ],
      "metadata": {
        "id": "oEp1wfadIj05"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Writing file do drive\n",
        "#df.to_csv(\"processed_data.csv\", index=False)\n",
        "#path = \"/content/processed_data.csv\""
      ],
      "metadata": {
        "id": "UW0O51b-IMrI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.3 Reformatting to jsonl \n"
      ],
      "metadata": {
        "id": "VJftXkxeX6hu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using OpenAI’s command line tool to ensure that the preprocessing was done correctly."
      ],
      "metadata": {
        "id": "eavaD7kMQWTv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# From OpenAI's own docu\n",
        "!openai tools fine_tunes.prepare_data -f $path"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oWEME0ukEb_7",
        "outputId": "8f1b307b-2ad9-4e49-aae7-3b1c7fbae8b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Analyzing...\n",
            "\n",
            "- Based on your file extension, your file is formatted as a CSV file\n",
            "- Your file contains 402 prompt-completion pairs\n",
            "- `completion` column/key should not contain empty strings. These are rows: [186]\n",
            "- There are 9 duplicated prompt-completion sets. These are rows: [95, 112, 149, 150, 168, 248, 276, 383, 397]\n",
            "- Your data does not contain a common separator at the end of your prompts. Having a separator string appended to the end of the prompt makes it clearer to the fine-tuned model where the completion should begin. See https://beta.openai.com/docs/guides/fine-tuning/preparing-your-dataset for more detail and examples. If you intend to do open-ended generation, then you should leave the prompts empty\n",
            "- All completions end with suffix `\\n`\n",
            "  WARNING: Some of your completions contain the suffix `\n",
            "` more than once. We suggest that you review your completions and add a unique ending\n",
            "- The completion should start with a whitespace character (` `). This tends to produce better results due to the tokenization we use. See https://beta.openai.com/docs/guides/fine-tuning/preparing-your-dataset for more details\n",
            "\n",
            "Based on the analysis we will perform the following actions:\n",
            "- [Necessary] Your format `CSV` will be converted to `JSONL`\n",
            "- [Necessary] Remove 1 rows with empty completions\n",
            "- [Recommended] Remove 9 duplicate rows [Y/n]: Y\n",
            "- [Recommended] Add a suffix separator `\\n\\n###\\n\\n` to all prompts [Y/n]: Y\n",
            "- [Recommended] Add a whitespace character to the beginning of the completion [Y/n]: Y\n",
            "\n",
            "\n",
            "Your data will be written to a new JSONL file. Proceed [Y/n]: Y\n",
            "\n",
            "Wrote modified file to `/content/processed_data_prepared (1).jsonl`\n",
            "Feel free to take a look!\n",
            "\n",
            "Now use that file when fine-tuning:\n",
            "> openai api fine_tunes.create -t \"/content/processed_data_prepared (1).jsonl\"\n",
            "\n",
            "After you’ve fine-tuned a model, remember that your prompt has to end with the indicator string `\\n\\n###\\n\\n` for the model to start generating completions, rather than continuing with the prompt. Make sure to include `stop=[\"\\n\"]` so that the generated texts ends at the expected place.\n",
            "Once your model starts training, it'll approximately take 17.53 minutes to train a `curie` model, and less for `ada` and `babbage`. Queue will approximately take half an hour per job ahead of you.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The reformatting tool did five things to the data:\n",
        "1. Changed it from .csv to .jsonl\n",
        "2. Removed one row with empty completions\n",
        "3. Spottet and removed 9 duplicate rows\n",
        "4. Added a suffix sepperator '\\n\\n###\\n\\n' to all prompts\n",
        "5. Added a whitespace character to the beginning of all completions"
      ],
      "metadata": {
        "id": "_6UiA7MOI9uf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Create a fine-tuned model"
      ],
      "metadata": {
        "id": "VRASJvRCJUla"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following assumes you've already prepared training data following the above instructions.\n",
        "\n",
        "Start your fine-tuning job using the OpenAI CLI:"
      ],
      "metadata": {
        "id": "q7YWUFgvJZWP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Navigate to the location of the jsonl file (is located in the 'data' folder in the github repository)\n",
        "path = \"/content/processed_data_prepared.jsonl\"\n",
        "train_file = path"
      ],
      "metadata": {
        "id": "-xePCiTibVZJ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.1 Setting hyperparameters"
      ],
      "metadata": {
        "id": "sBc2-wRcA6Zr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = 'davinci'  # can be ada, babbage, curie or davinci\n",
        "n_epochs = 4\n",
        "batch_size = 4\n",
        "learning_rate_multiplier = 0.1\n",
        "prompt_loss_weight = 0.1"
      ],
      "metadata": {
        "id": "UNstAk3cOhYB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.2 Training"
      ],
      "metadata": {
        "id": "mx5ZUOemX8O5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!openai api fine_tunes.create \\\n",
        "    -t $train_file \\\n",
        "    -m $model \\\n",
        "    --n_epochs $n_epochs \\\n",
        "    --batch_size $batch_size \\\n",
        "    --learning_rate_multiplier $learning_rate_multiplier \\\n",
        "    --prompt_loss_weight $prompt_loss_weight"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M41b7BFSbOBN",
        "outputId": "5d59098c-079c-40ea-98e6-c961827d3d1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rUpload progress:   0% 0.00/403k [00:00<?, ?it/s]\rUpload progress: 100% 403k/403k [00:00<00:00, 785Mit/s]\n",
            "Uploaded file from /content/processed_data_prepared.jsonl: file-qm5kNN8ibOkozvLdnmx7IAYW\n",
            "Created fine-tune: ft-qEzBbVEURqzI2svgwhbSd0UC\n",
            "Streaming events until fine-tuning is complete...\n",
            "\n",
            "(Ctrl-C will interrupt the stream, but not cancel the fine-tune)\n",
            "[2022-12-28 10:03:12] Created fine-tune: ft-qEzBbVEURqzI2svgwhbSd0UC\n",
            "[2022-12-28 10:04:31] Fine-tune costs $19.52\n",
            "[2022-12-28 10:04:31] Fine-tune enqueued. Queue number: 0\n",
            "[2022-12-28 10:04:33] Fine-tune started\n",
            "[2022-12-28 10:09:20] Completed epoch 1/4\n",
            "\n",
            "Stream interrupted (client disconnected).\n",
            "To resume the stream, run:\n",
            "\n",
            "  openai api fine_tunes.follow -i ft-qEzBbVEURqzI2svgwhbSd0UC\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Resuming interupted training\n",
        "!openai api fine_tunes.follow -i ft-qEzBbVEURqzI2svgwhbSd0UC"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oCquFib24txl",
        "outputId": "906ae243-7595-489d-c3e7-8ad5d0c36e1f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: openai: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Inferencing"
      ],
      "metadata": {
        "id": "O6svYNzcBB4X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import openai\n",
        "\n",
        "# For this step, is was necesarry to pass the API key as a text file instead of a direct paste. \n",
        "def open_file(filepath):\n",
        "  with open(filepath, 'r', encoding='utf-8') as infile:\n",
        "    return infile.read()\n",
        "# Attaching the API key\n",
        "openai.api_key = open_file(\"/content/openaiAPI.txt\")\n",
        "\n",
        "# The following function takes a prompt (of type string) as input and outputs the text generation\n",
        "def gpt3(prompt):\n",
        "  response = openai.Completion.create(\n",
        "  model=\"davinci:ft-aarhus-university-2022-12-28-10-18-17\", # insert your finetuned model name here\n",
        "  prompt=prompt,\n",
        "  #stop = \"\\n\",\n",
        "  temperature=0.7, \n",
        "  max_tokens=180, # determines the number of tokens the model outputs\n",
        "  top_p=1, \n",
        "  frequency_penalty=0,\n",
        "  presence_penalty=0\n",
        "  )\n",
        "  text = response['choices'][0]['text'].strip()\n",
        "  return text"
      ],
      "metadata": {
        "id": "GTuk7cXl6jjv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* ```temperature``` is used to control the randomness or creativity of the model's output. A high temperature will cause the model to generate more diverse and creative output. A low temperature will cause the model to be more conservative and stick more closely to the patterns it has learned, resulting in more predictable but potentially less interesting output.\n",
        "\n",
        "* The ```top_p``` parameter can be useful for controlling the diversity and coherence of the model's output, particularly when used in combination with the temperature parameter. It can be adjusted to find a balance between the model's creativity and its ability to produce accurate and appropriate responses.\n",
        "\n",
        "*  The ```frequency_penalty``` parameter is used to control the diversity of the model's output by penalizing the probability of generating tokens that have been generated frequently in the past.\n",
        "\n",
        "* Similarly, the ```presence_penalty``` parameter is used to control the diversity of the model's output by penalizing the probability of generating tokens that are too similar to previously generated tokens."
      ],
      "metadata": {
        "id": "1PahIrmXO7FC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Outputs a list of the prompts:"
      ],
      "metadata": {
        "id": "_tA5HDmnP486"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GBQbBb5gXREj",
        "outputId": "4a4c1ceb-9921-46a1-8fb3-6509413c2504"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Det er så yndigt at følges ad for to, som gerne vil sammen være, da er med glæden man dobbelt glad og halvt om sorgen så tung at bære; ja, det er gammen //: at rejse sammen, :// når fjederhammen //: er kærlighed. ://\\n',\n",
              " 'Det er hvidt herude,kyndelmisse slår sin knudeovermåde hvas og hård,hvidt forneden, hvidt foroven,pudret tykt står træ i skovensom udi min abildgård.\\n',\n",
              " 'Der er noget i luften, jeg ved ikke hvad,som forår, skønt skoven har mistet hvert blad.Der er noget i luften, som rosernes duften,som fuglenes fryd,skønt rosen er falmet, og fuglen er draget mod syd.\\n',\n",
              " 'Vær velkommen, Herrens år,og velkommen herhid!Julenat, da vor Herre blev fød,da tændte sig lyset i mørkets skød.Velkommen, nytår, og velkommen her!\\n',\n",
              " 'Vem kan segla förutan vind?Vem kan ro utan åror?Vem kan skiljas från vännen sinutan att fälla tårar?\\n',\n",
              " 'Danmark, nu blunder den lyse natbag ved din seng, når du sover.Gøgen kukker i skov og krat,Vesterhavet og Kattegatsynger, imens det dugger,sagte som sang ved vugger.\\n',\n",
              " \"En rose så jeg skydeop af den frosne jord,alt som os fordum spå'deprofetens trøsteord.Den rose spired fremmidt i den kolde vinterom nat ved Betlehem.\\n\",\n",
              " 'Hvem sidder dér bag skærmenmed klude om sin hånd,med læderlap for øjetog om sin sko et bånd?Det er såmænd Jens vejmand,der af sin sure nødmed hamren må forvandlede hårde sten til brød.\\n',\n",
              " '//: Det var en lørdag aften, jeg sad og vented dig, - :// //: du loved mig at komme vist, men kom dog ej til mig! ://\\n',\n",
              " 'Et solstrejf i en vandpyt,et lille kindkys af en vindog sivet, der nynner,at livet begyndersit spind i dit sind.\\nEt sølvõøjt fra en trætop,en svag tagfat-lyd af en kat,en rislen i bækken,en hvislen i hækken,der si’r, at det ikke mer’ er nat.\\nDugvåd ligger engen,jomfru Daggry går til ro.Dagen står Puk-kåd ud af sengenog går over solens bro.\\nOg os to?Hvad med os to?Ja, hvad med os to dig og mig?Jeg danser og danser og standserog sanser kun dig.Hvorfor løb du dog din vej?\\nKom igen, kom igen,kom igen du min elskede ven.Kom igen, kom igen.Hvor du ønsker det, danser vi hen.\\nKom, lad os danse,alt kan der ske.Kom, lad os danse,lad os danse, lad os le.\\n',\n",
              " \"Amazing grace how sweet the soundThat saved a wretch like me!I once was lost, but now I'm found,Was blind but now I see.\\n\"]"
            ]
          },
          "metadata": {},
          "execution_count": 139
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the function above to generate inferences on the prompts stores in the 'prompts' list."
      ],
      "metadata": {
        "id": "YGIop8juKIHT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gpt3(prompts[10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "cgY-k9YVQI0b",
        "outputId": "5ebef805-2246-4f4b-8c83-505710c5445b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"2. 'Twas grace that taught my heart to fear,And grace my fears relieved;How precious did that grace appearThe hour I first believed.\\n\\n3. Through many dangers, toils and snaresI have already come;'Tis grace hath brought me safe thus far,And grace will lead me home.\\n\\n4. The Lord has promised good to me,His word my hope secures;He will my shield and portion beAs long as life endures.\\n\\n5. Yea, when this flesh and heart shall fail,And mortal life shall cease,I shall possess, within the veil,A life of joy and peace.\\n\\n6. The earth shall soon dissolve like snow,The sun forbear to shine;But God, who called me here below,Will be forever mine.\\n\\n7. When we've been there\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 178
        }
      ]
    }
  ]
}